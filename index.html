<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Cristian Canton</title>

  <meta name="author" content="Cristian Canton">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Cristian Canton</name>
              </p>
              <p>I am senior engineering/research manager at <a href="https://ai.facebook.com">Facebook AI</a>, where I support multiple teams related to AI robustness, fairness and legitimacy.
              </p>
              <p>
              Erd≈ës number: <strong>3</strong></a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:cristian.canton@gmail.com">Email</a> &nbsp/&nbsp
                <!--<a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp-->
                <!--<a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp-->
                <a href="https://scholar.google.com/citations?hl=en&user=akzoGjUAAAAJ" target="_blank">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/cristiancanton" target="_blank">Twitter</a>
                <!--<a href="https://github.com/jonbarron/">Github</a>-->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/CristianCanton.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/CristianCanton_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, machine learning, optimization, and image processing.
                Much of my research is about inferring the physical world (shape, motion, color, light, etc) from images.
                <!-- Representative papers are <span class="highlight">highlighted</span>. -->
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!---- 2021 ------>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <center><a href="https://arxiv.org/abs/2011.12902" target="_blank">
                <img src="images/2021-GrayBoxAttacks.png" style="border-style: none"></a></center>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2011.12902" target="_blank">
                <papertitle>Adversarial Evaluation of Multimodal Models under Realistic Gray Box Assumption</papertitle>
              </a>
              <br>
              Ivan Evtimov, Russ Howes, Brian Dolhansky, Hamed Firooz, <strong>Cristian Canton</strong>
              <br>
              <em>Under review</em>, 2021.
              <br>
              <a href="data/2021-GrayBoxAttacks.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Multimodal understanding (image+text) can be attacked in a gray-box fashing. Examples on hate memes included!</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <center><a href="https://arxiv.org/abs/2011.09473" target="_blank">
                <img src="images/2021-HashCollision.png" style="border-style: none"></a></center>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2011.09473" target="_blank">
                <papertitle>Adversarial collision attacks on image hashing functions</papertitle>
              </a>
              <br>
              Brian Dolhansky, <strong>Cristian Canton</strong>
              <br>
              <em>Under review</em>, 2021.
              <br>
              <a href="data/2021-HashCollision.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Hashing algorithms (in general) can be easily compromised by introducing collisions.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <center><a href="https://arxiv.org/abs/2104.02821" target="_blank">
                <img src="images/2021-CasualConversations.png" style="border-style: none"></a></center>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2104.02821" target="_blank">
                <papertitle>Towards measuring fairness in AI: the Casual Conversations dataset</papertitle>
              </a>
              <br>
              Caner Hazirbas, Joanna Bitton, Brian Dolhansky, Jacqueline Pan,<br>
              Albert Gordo, <strong>Cristian Canton</strong>
              <br>
              <em>Computer Vision and Pattern Recognition Conference (CVPR). Workshop on Responsible Computer Vision</em>, 2021.<br>
              <em><b>Journal</b> - Under review</em>, 2021.
              <br>
              <a href="data/2021-CasualConversations.bib.txt" target="_blank">[bibtex]</a>&nbsp;&nbsp;<a href="https://ai.facebook.com/datasets/casual-conversations-dataset/" target="_blank">[dataset]</a>
              <p></p>
              <p>TL;DR. A large dataset (10TB, 30+ days of video, 3K people, 15 mins per person) featuring a diversity of actors with age, gender and skin tone annotations.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <center><a href="https://arxiv.org/abs/2011.09957" target="_blank">
                <img src="images/2021-CVPRW-Forensics.png" style="border-style: none"></a></center>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2011.09957" target="_blank">
                <papertitle>Adversarial Threats to DeepFake Detection: A Practical Perspective</papertitle>
              </a>
              <br>
              Paarth Neekhara, Brian Dolhansky, Joanna Bitton, <strong>Cristian Canton</strong>
              <br>
              <em>Computer Vision and Pattern Recognition Conference (CVPR). Workshop on Media Forensics</em>, 2021.
              <br>
              <a href="data/2021-CVPRW-Forensics.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Creation of adversarial additive patterns against open sourced deepfake detection systems turns out to be very effective.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <center><a href="https://arxiv.org/abs/1912.06895" target="_blank">
                <img src="images/2021-WACV.png" style="border-style: none"></a></center>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1912.06895" target="_blank">
                <papertitle>Deep Poisoning: Towards Robust Image Data Sharing against Visual Disclosure</papertitle>
              </a>
              <br>
              Hao Guo, Brian Dolhansky, Eric Hsin, Phong Dinh, Song Wang, <strong>Cristian Canton</strong>
              <br>
              <em>IEEE Winter Conference on Applications of Computer Vision (WACV)</em>, 2021.
              <br>
              <a href="data/2021-WACV.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Create embeddings that are privacy preserving thus non-reversible to obtain the original image.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <center><a href="https://arxiv.org/abs/2009.10311" target="_blank">
                <img src="images/2021-ACM.png" style="border-style: none"></a></center>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2009.10311" target="_blank">
                <papertitle>Preserving Integrity in Online Social Networks</papertitle>
              </a>
              <br>
              Alon Halevy, <strong>Cristian Canton</strong>, Hao Ma, Umut Ozertem, Patrick Pantel,<br>
              Marzieh Saeidi, Fabrizio Silvestri, Ves Stoyanov
              <br>
              <em><b>Journal</b> - Communications of the ACM (CACM)</em>, 2021.
              <br>
              <a href="data/2021-ACM.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. A comprehensive survey of AI techniques applied in social networks to preserve integrity and preserve users from bad experiences.</p>
            </td>
          </tr>
          <!---- 2020 ------>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <center><a href="https://arxiv.org/abs/2006.07397" target="_blank">
                <img src="images/2019-DFDC.png" style="border-style: none"></a></center>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2006.07397" target="_blank">
                <papertitle>The Deepfake Detection Challenge Dataset</papertitle>
              </a>
              <br>
              Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang,<br>
              <strong>Cristian Canton</strong>
              <br>
              <em>Facebook AI Technical Report</em>, 2020.
              <br>
              <a href="data/2019-DFDC.bib.txt" target="_blank">[bibtex]</a>&nbsp;&nbsp;<a href="https://ai.facebook.com/datasets/dfdc" target="_blank">[dataset]</a>&nbsp;&nbsp;<a href="https://www.kaggle.com/c/deepfake-detection-challenge" target="_blank">[Kaggle competition]</a>
              <p></p>
              <p>TL;DR. Up to today, the largest deepfake dataset. A $1M competition!</p>
            </td>
          </tr>
          <!---- 2019 ------>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <center><a href="https://arxiv.org/abs/1910.08854" target="_blank">
                <img src="images/2019-DFDCPreview.png" style="border-style: none"></a></center>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1910.08854" target="_blank">
                <papertitle>The Deepfake Detection Challenge (DFDC) Preview Dataset</papertitle>
              </a>
              <br>
              Brian Dolhansky, Russ Howes, Ben Pflaum, Nicole Baram, <strong>Cristian Canton</strong>
              <br>
              <em>Facebook AI Technical Report</em>, 2019.
              <br>
              <a href="data/2019-DFDCPreview.bib.txt" target="_blank">[bibtex]</a>&nbsp;&nbsp;<a href="https://dfdc.ai" target="_blank">[dataset]</a>&nbsp;&nbsp;<a href="https://ai.facebook.com/blog/deepfake-detection-challenge-aws-and-new-academics-join/" target="_blank">[blog post]</a>
              <p></p>
              <p>TL;DR. A small preview version of the DFDC dataset to come later.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <center><a href="https://arxiv.org/abs/1910.02334" target="_blank">
                <img src="images/2019-NeurIPS.png" style="border-style: none"></a></center>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1910.02334" target="_blank">
                <papertitle>Hate Speech in Pixels: Detection of Offensive Memes towards Automatic Moderation</papertitle>
              </a>
              <br>
              Benet Oriol Sabat, <strong>Cristian Canton</strong>, Xavier Gir√≥‚Äëi‚ÄëNieto
              <br>
              <em>NeurIPS AI for Social Good Workshop</em>, Vancouver, (Canada), 2019.
              <br>
              <a href="data/2019-NeurIPS.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Multimodal understanding of memes can allow better detection of image+text hate speech.</p>
            </td>
          </tr>
          <!---- 2018 ------>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <center><a href="https://arxiv.org/abs/1709.01424" target="_blank">
                <img src="images/2018-CVIU.png" style="border-style: none"></a></center>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1709.01424" target="_blank">
                <papertitle>Towards social pattern characterization in egocentric photo-streams</papertitle>
              </a>
              <br>
              Maedeh Aghaei, Mariella Dimiccoli, <strong>Cristian Canton</strong>, Petia Radeva
              <br>
              <em><b>Journal</b> - Computer Vision and Image Understanding (CVIU)</em>, 2018.
              <br>
              <a href="data/2018-CVIU.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. xxx.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <center><a href="https://arxiv.org/abs/1712.03999" target="_blank">
                <img src="images/2018-CVPR.png" style="border-style: none"></a></center>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1712.03999" target="_blank">
                <papertitle>Eye In-Painting with Exemplar Generative Adversarial Networks</papertitle>
              </a>
              <br>
              Brian Dolhansky, <strong>Cristian Canton</strong>
              <br>
              <em>Computer Vision and Pattern Recognition Conference (CVPR)</em>, Salt Lake City, (USA), 2018.
              <br>
              <a href="data/2018-CVPR.bib.txt" target="_blank">[bibtex]</a>&nbsp;&nbsp;<a href="https://drive.google.com/open?id=1UgZeFc4CglCc5rsMvlKL1OdWEEDzkpSa" target="_blank">[poster]</a>
              <p></p>
              <p>TL;DR. Given an example of how your eyes look, this algorithm can take a pic with your eyes closed and open them. GAN-powered.</p>
            </td>
          </tr>
          <!---- 2017 ------>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <center><a href="https://arxiv.org/abs/1709.05775" target="_blank">
                <img src="images/2017-ICCV.png" style="border-style: none"></a></center>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1709.05775" target="_blank">
                <papertitle>Social Style Characterization from Egocentric Photo-Streams</papertitle>
              </a>
              <br>
              Maedeh Aghaei, Mariella Dimiccoli, <strong>Cristian Canton</strong>, Petia Radeva
              <br>
              <em>International Conference on Computer Vision (ICCV). Workshop on Egocentric Percetion, Interaction and Computing</em>, Venice (Italy), 2017.
              <br>
              <a href="data/2017-ICCV.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Use an ego-camera, process multimodal inputs to do long time span scene understanding, specially for social style signals.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <center><a href="https://arxiv.org/abs/1707.04092" target="_blank">
                <img src="images/2017-CVPR2.png" style="border-style: none"></a></center>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1707.04092" target="_blank">
                <papertitle>Disentangling Motion, Foreground and Background Features in Videos</papertitle>
              </a>
              <br>
              Xunyu Lin, V√≠ctor Campos, Xavier Gir√≥‚Äëi‚ÄëNieto, Jordi Torres, <strong>Cristian Canton</strong>
              <br>
              <em>Computer Vision and Pattern Recognition Conference (CVPR). Brave New Motion Representations Workshop</em>, Honolulu (USA), 2017.
              <br>
              <a href="data/2017-CVPR2.bib.txt" target="_blank">[bibtex]</a>&nbsp;&nbsp;<a href="https://allenovo.github.io/cvprw17_webpage/" target="_blank">[source code]</a>&nbsp;&nbsp;<a href="https://drive.google.com/open?id=0B5fAXmGH22uZbkhXN2hJRVprdXc" target="_blank">[poster]</a>
              <p></p>
              <p>TL;DR. An unsupervised framework to extract semantically rich features for video representation inspired by how the human visual system groups objects based on motion cues.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <center><a href="https://arxiv.org/abs/1701.01081" target="_blank">
                <img src="images/2017-CVPR1.png" style="border-style: none"></a></center>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1701.01081" target="_blank">
                <papertitle>SalGAN: Visual Saliency Prediction with Generative Adversarial Networks</papertitle>
              </a>
              <br>
              <a href="https://junting.github.io/" target="_blank">Junting Pan</a>,
              <strong>Cristian Canton</strong>, Kevin McGuinness, Noel O'Connor, Jordi Torres, Elisa Sayrol, Xavier Gir√≥‚Äëi‚ÄëNieto
              <br>
              <em>Computer Vision and Pattern Recognition Conference (CVPR). Scene Understanding Workshop</em>, Honolulu (USA), 2017.
              <br>
              <a href="data/2017-CVPR1.bib.txt" target="_blank">[bibtex]</a>&nbsp;&nbsp;<a href="https://github.com/imatge-upc/salgan" target="_blank">[source code]</a>&nbsp;&nbsp;<a href="https://drive.google.com/open?id=0B5fAXmGH22uZVDdGMm5RcUs0VE0" target="_blank">[poster]</a>
              <p></p>
              <p>TL;DR. Estimate of saliency using a GAN, probably the first article to do it!</p>
            </td>
          </tr>
          <!---- 2016 ------>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <center><a href="https://arxiv.org/abs/1608.01041" target="_blank">
                <img src="images/2016-ICMI2.png" style="border-style: none"></a></center>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1608.01041" target="_blank">
                <papertitle>Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution</papertitle>
              </a>
              <br>
              Emad Barsoum,
              <a href="https://www.microsoft.com/en-us/research/people/chazhang/" target="_blank">Cha Zhang</a>,
              <strong>Cristian Canton</strong>,
              <a href="https://en.wikipedia.org/wiki/Zhengyou_Zhang" target="_blank">Zhengyou Zhang</a>
              <br>
              <em>ACM International Conference on Multimodal Interaction (ICMI)</em>, Tokio (Japan), 2016.
              <br>
              <a href="data/2016-ICMI2.bib.txt" target="_blank">[bibtex]</a>&nbsp;&nbsp;<a href="https://github.com/Microsoft/FERPlus" target="_blank">[dataset]</a>
              <p></p>
              <p>TL;DR. Some strategies to combine multiple subjective labels for human emotions in a way that CNNs can be trained effectively.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <center><a href="https://drive.google.com/open?id=0B5fAXmGH22uZZ3dQSE5GRmt3UHM" target="_blank">
                <img src="images/2016-ICMI.png" style="border-style: none"></a></center>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZZ3dQSE5GRmt3UHM" target="_blank">
                <papertitle>Emotion Recognition in the Wild from Videos using Images</papertitle>
              </a>
              <br>
              <a href="https://cs-people.bu.edu/sbargal/" target="_blank">Sarah Bargal</a>,
              Emad Barsoum,
              <strong>Cristian Canton</strong>,
              <a href="https://www.microsoft.com/en-us/research/people/chazhang/" target="_blank">Cha Zhang</a>
              <br>
              <em>ACM International Conference on Multimodal Interaction (ICMI). Workshop on Facial Expressions in the Wild</em>, Tokio (Japan), 2016.
              <br>
              <a href="data/2016-ICMI.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Ensambles of CNN embeddings allow for high accuracy emotion recognition on videos.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <center><a href="https://arxiv.org/abs/1604.07866" target="_blank"><img src="images/2016-CVPRW.png" style="border-style: none"></a></center>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1604.07866" target="_blank">
                <papertitle>Learning by Tracking: Siamese CNN for Robust Target Association</papertitle>
              </a>
              <br>
              <a href="https://dvl.in.tum.de/team/lealtaixe" target="_blank">Laura Leal-Taix√©</a>,
              <strong>Cristian Canton</strong>,
              <a href="https://igp.ethz.ch/personen/person-detail.html?persid=143986" target="_blank">Konrad Schindler</a>
              <br>
              <em>Computer Vision and Pattern Recognition Conference (CVPR). DeepVision Workshop</em>, Las Vegas (USA), 2016.
              <br>
              <a href="data/2016-CVPRW.bib.txt" target="_blank">[bibtex]</a>&nbsp;&nbsp;<a href="https://drive.google.com/open?id=0B5fAXmGH22uZeVFWVHZZWTBJQlU" target="_blank">[poster]</a>
              <p></p>
              <p>TL;DR. While tracking multiple targets, comparing detections using a Siamese network improves overall performance.</p>
            </td>
          </tr>
          <!---- 2012 ------>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://www.routledge.com/Skill-Training-in-Multimodal-Virtual-Environments/Bergamasco-Bardy-Gopher/p/book/9781439878958" target="_blank"><img src="images/2012-SKILLS.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.routledge.com/Skill-Training-in-Multimodal-Virtual-Environments/Bergamasco-Bardy-Gopher/p/book/9781439878958" target="_blank">
                <papertitle>Motion Capture Technologies for Pose Estimation</papertitle>
              </a>
              <br>
              <strong>Cristian Canton</strong>, P. Smyth
              <br>
              <em><b>Book chapter</b> - Skill Training in Multimodal Virtual Environments (Taylor & Francis Ed.)</em>, 2012.
              <br>
              <a href="data/2012-SKILLS.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. xxx.</p>
            </td>
          </tr>
          <!---- 2011 ------>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZQWJvSmNVdEU3MjA" target="_blank"><img src="images/2011-EURASIP.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZQWJvSmNVdEU3MjA" target="_blank">
                <papertitle>Multi-Camera Multi-Object Voxel Based Monte Carlo 3D Tracking Strategies</papertitle>
              </a>
              <br>
              <strong>Cristian Canton</strong>,
              <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>,
              <a href="https://imatge.upc.edu/web/people/montse-pardas">Montse Pard√†s</a>,
              Enric Monte
              <br>
              <em><b>Journal</b> - EURASIP Journal on Advances on Signal Processing</em>, 2011.
              <br>
              <a href="data/2011-EURASIP.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. xxx.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZU1Q1a0NkZnBZLVE" target="_blank"><img src="images/2011-CVIU.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZU1Q1a0NkZnBZLVE" target="_blank">
                <papertitle>Human Motion Capture Using Scalable Body Models</papertitle>
              </a>
              <br>
              <strong>Cristian Canton</strong>,
              <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>,
              <a href="https://imatge.upc.edu/web/people/montse-pardas">Montse Pard√†s</a>
              <br>
              <em><b>Journal</b> - Computer Vision and Image Understanding (CVIU)</em>, 2011.
              <br>
              <a href="data/2011-CVIU.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. xxx.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZelUzdThYMm9Mak0" target="_blank"><img src="images/2011-EURASIP2.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZelUzdThYMm9Mak0" target="_blank">
                <papertitle>Acoustic Event Detection based on Feature-Level Fusion of Audio and Video Modalities</papertitle>
              </a>
              <br>
              Taras Butko, <strong>Cristian Canton</strong>, Carlos Segura,
              Xavier Gir√≥-i-Nieto, Climent Nadeu,<br>
              Javier Hernando, <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>
              <br>
              <em><b>Journal</b> - EURASIP Journal on Advances on Signal Processing</em>, 2011.
              <br>
              <a href="data/2011-EURASIP2.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. xxx.</p>
            </td>
          </tr>
          <!---- 2010 ------>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://www.amazon.co.uk/gp/product/0123748259?ie=UTF8&tag=mozaiediti-21&linkCode=as2&camp=1634&creative=6738&creativeASIN=0123748259" target="_blank"><img src="images/2010-HCI.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.amazon.co.uk/gp/product/0123748259?ie=UTF8&tag=mozaiediti-21&linkCode=as2&camp=1634&creative=6738&creativeASIN=0123748259" target="_blank">
                <papertitle>Image and Video Processing Tools for HCI</papertitle>
              </a>
              <br>
              <a href="https://imatge.upc.edu/web/people/montse-pardas">Montse Pard√†s</a>,
              <a href="https://imatge.upc.edu/web/people/veronica-vilaplana">Ver√≥nica Vilaplana</a>,
              <strong>Cristian Canton</strong>
              <br>
              <em><b>Book chapter</b> - Multimodal Signal Processing: Theory and applications for human-computer interaction (Academic Press)</em>, 2010.
              <br>
              <a href="data/2010-HCI.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. xxx.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZRldFVkkzenpHSzQ" target="_blank"><img src="images/2010-EUSIPCO.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZRldFVkkzenpHSzQ" target="_blank">
                <papertitle>Virtual View Appearance Representation for Human Motion Analysis in Multi-View Environments</papertitle>
              </a>
              <br>
              Adolfo L√≥pez,
              <strong>Cristian Canton</strong>,
              <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>,
              <br>
              <em>European Signal Processing Conference (EUSIPCO)</em>, Aalborg (Denmark), 2010.
              <br>
              <a href="data/2010-EUSIPCO.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Generating virtual orthogonal views derived from 3D voxels allow better action recognition on 2D projections.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZTHhJcmJwaTN3TGM" target="_blank"><img src="images/2010-EURASIP.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZTHhJcmJwaTN3TGM" target="_blank">
                <papertitle>Marker-based Human Motion Capture in Multi-View Sequences</papertitle>
              </a>
              <br>
              <strong>Cristian Canton</strong>,
              <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>,
              <a href="https://imatge.upc.edu/web/people/montse-pardas">Montse Pard√†s</a>
              <br>
              <em><b>Journal</b> - EURASIP Journal on Advances on Signal Processing</em>, 2010.
              <br>
              <a href="data/2010-EURASIP.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. xxx.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZdHBCNnEycXdMU1E" target="_blank"><img src="images/2010-CVPRWBiometrics.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZdHBCNnEycXdMU1E" target="_blank">
                <papertitle>Spatio-Temporal Alignment and Hyperspherical Radon Transform for 3D Gait Recognition in Multi-View Environments</papertitle>
              </a>
              <br>
              <strong>Cristian Canton</strong>,
              <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>,
              <a href="https://imatge.upc.edu/web/people/montse-pardas">Montse Pard√†s</a>
              <br>
              <em>IEEE International Computer Vision and Pattern Recognition Conference (CVPR). Biometrics Workshop</em>, San Francisco (USA), 2010.
              <br>
              <a href="data/2010-CVPRWBiometrics.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Understanding actions directly on 3D+time hypervolumes using the Radon transform.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZZl9rODdBX3pHVlU" target="_blank"><img src="images/2010-CVPRW.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZZl9rODdBX3pHVlU" target="_blank">
                <papertitle>Real-Time 3D Multi-Person Tracking Using Monte Carlo Surface Sampling</papertitle>
              </a>
              <br>
              <strong>Cristian Canton</strong>,
              <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>,
              <a href="https://imatge.upc.edu/web/people/montse-pardas">Montse Pard√†s</a>
              <br>
              <em>IEEE International Computer Vision and Pattern Recognition Conference (CVPR). Computer Vision for Games Workshop</em>, San Francisco (USA), 2010.
              <br>
              <a href="data/2010-CVPRW.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Conditioning particle filtering to run on surface of volumes ensure better tracking through a new occupancy resampling strategy.</p>
            </td>
          </tr>

          <!---- 2009 ------>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="papers/2009-CHILBook_Head-Canton.pdf" target="_blank"><img src="images/2009-HeadBook.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="papers/2009-CHILBook_Head-Canton.pdf" target="_blank">
                <papertitle>Image and Video Processing Tools for HCI</papertitle>
              </a>
              <br>
              Michael Voit, Nicolas Gourier, <strong>Cristian Canton</strong>, Oswald Lanz,<br>
              Rainer Stiefelhagen, Roberto Brunelli
              <br>
              <em><b>Book chapter</b> - Computers in the Human Interaction Loop (Springer-Verlag)</em>, 2009.
              <br>
              <a href="data/2009-HeadBook.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. xxx.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZWFQ2NmRzdkhNbGs" target="_blank"><img src="images/2009-ICIP.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZWFQ2NmRzdkhNbGs" target="_blank">
                <papertitle>Towards a Low Cost Multi-Camera Marker Based Human Motion Capture System</papertitle>
              </a>
              <br>
              <strong>Cristian Canton</strong>,
              <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>,
              <a href="https://imatge.upc.edu/web/people/montse-pardas">Montse Pard√†s</a>
              <br>
              <em>IEEE International Conference on Image Processing (ICIP)</em>, Cairo (Egypt), 2009.
              <br>
              <a href="data/2009-ICIP.bib.txt" target="_blank">[bibtex]</a>&nbsp;&nbsp;<a href="https://drive.google.com/open?id=0B5fAXmGH22uZVGZBQ1c0cDRqTzA" target="_blank">[poster]</a>
              <p></p>
              <p>TL;DR. Marker based motion tracking using annealed particle filtering.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZLWVuVlRlLU16Unc" target="_blank"><img src="images/2009-Interspeech.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZLWVuVlRlLU16Unc" target="_blank">
                <papertitle>Improving Detection of Acoustic Events Using Audiovisual Data and Feature Level Fusion</papertitle>
              </a>
              <br>
              Taras Butko, <strong>Cristian Canton</strong>, Carlos Segura,
              Xavier Gir√≥-i-Nieto, Climent Nadeu,<br>
              Javier Hernando, <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>
              <br>
              <em>Interspeech</em>, Brighton (UK), 2009.
              <br>
              <a href="data/2009-Interspeech.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Acoustic event detection using multiple cues and specific features using HMM.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZYUFNeXhYYWljWDA" target="_blank"><img src="images/2009-CVPRW.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZYUFNeXhYYWljWDA" target="_blank">
                <papertitle>Multimodal Acoustic Event Detection Towards Scene Understanding</papertitle>
              </a>
              <br>
              <strong>Cristian Canton</strong>, Taras Butko, Carlos Segura,
              Xavier Gir√≥-i-Nieto, Climent Nadeu,<br>
              Javier Hernando, <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>
              <br>
              <em>IEEE International Conference on Computer Vision and Pattern Recognition (CVPR). Human Communicative Behavior Analysis Workshop</em>, Miami (USA), 2009.
              <br>
              <a href="data/2009-CVPRW.bib.txt" target="_blank">[bibtex]</a>&nbsp;&nbsp;<a href="https://drive.google.com/open?id=0B5fAXmGH22uZWU1oMzBqQnh0LWs" target="_blank">[poster]</a>
              <p></p>
              <p>TL;DR. Combination of motion cues, position and audio allows detection of events within a SmartRoom environment.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZbjBNYmdQek9LLWs" target="_blank"><img src="images/2009-3DTV.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZbjBNYmdQek9LLWs" target="_blank">
                <papertitle>Voxel based Annealed Particle Filtering for Markerless 3D Articulated Motion Capture</papertitle>
              </a>
              <br>
              <strong>Cristian Canton</strong>,
              <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>,
              <a href="https://imatge.upc.edu/web/people/montse-pardas">Montse Pard√†s</a>
              <br>
              <em>IEEE Conference on 3DTV</em>, Postdam (Germany), 2009.
              <br>
              <a href="data/2009-3DTV.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Markerless human body capture leveraging annealed particle filters prove to be effective and robust to occlusions.</p>
            </td>
          </tr>
          <!---- 2008 ------>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZZXJnMmlCWjdIbGc" target="_blank"><img src="images/2008-ICIP.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZZXJnMmlCWjdIbGc" target="_blank">
                <papertitle>Particle Filtering and Sparse Sampling for Multi-Person 3D Tracking</papertitle>
              </a>
              <br>
              <strong>Cristian Canton</strong>, Rosella Sblendido,
              <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>,
              <a href="https://imatge.upc.edu/web/people/montse-pardas">Montse Pard√†s</a>
              <br>
              <em>IEEE International Conference on Image Processing (ICIP)</em>, San Diego (USA), 2008.
              <br>
              <a href="data/2008-ICIP.bib.txt" target="_blank">[bibtex]</a>&nbsp;&nbsp;<a href="https://drive.google.com/open?id=0B5fAXmGH22uZMWxTVnVBcXhOQm8" target="_blank">[poster]</a>
              <p></p>
              <p>TL;DR. Adding color to 3D voxel representations improve multi-person tracking.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZUWczdlZMWjBJY1E" target="_blank"><img src="images/2008-Interspeech.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZUWczdlZMWjBJY1E" target="_blank">
                <papertitle>Fusion of Audio and Video Modalities for Detection of Acoustic Events</papertitle>
              </a>
              <br>
              Taras Butko, Andrey Temko, Climent Nadeu, <strong>Cristian Canton</strong>
              <br>
              <em>Interspeech</em>, Brisbane (Australia), 2008.
              <br>
              <a href="data/2008-Interspeech.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Leveraging spatial information of people in a SmartRoom improves acoustic event detection.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZWHlJTzB0Tlpjb1E" target="_blank"><img src="images/2008-MLMI.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZWHlJTzB0Tlpjb1E" target="_blank">
                <papertitle>Inclusion of Video Information for Detection of Acoustic Events using Fuzzy Integral</papertitle>
              </a>
              <br>
              Taras Butko, Andrey Temko, Climent Nadeu, <strong>Cristian Canton</strong>
              <br>
              <em>Workshop on Machine Learning and Multimodal Interaction (MLMI)</em>, Utrecht (The Netherlands), 2008.
              <br>
              <a href="data/2008-MLMI.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Multimodal data fusion towards acoustic events understanding in SmartRooms.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZa3puazNaaEFYMlE" target="_blank"><img src="images/2008-AMDO.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZa3puazNaaEFYMlE" target="_blank">
                <papertitle>Exploiting Structural Hierarchy in Articulated Objects towards Robust Motion Capture</papertitle>
              </a>
              <br>
              <strong>Cristian Canton</strong>,
              <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>,
              <a href="https://imatge.upc.edu/web/people/montse-pardas">Montse Pard√†s</a>
              <br>
              <em>Conference on Articulated Motion and Deformable Objects (AMDO)</em>, Mallorca (Spain), 2008.
              <br>
              <a href="data/2008-AMDO.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. New Montecarlo method applied to recover articulated structured (human body) exploiting hierarchies of complexity.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZZEFRTmxHMnhGVnM" target="_blank"><img src="images/2008-CVPRW.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZZEFRTmxHMnhGVnM" target="_blank">
                <papertitle>Multimodal Real-Time Focus of Attention Estimation in SmartRooms</papertitle>
              </a>
              <br><strong>Cristian Canton</strong>,
              Carlos Segura,
              <a href="https://imatge.upc.edu/web/people/montse-pardas">Montse Pard√†s</a>,
              <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>,
              Javier Hernando
              <br>
              <em>IEEE International Conference on Computer Vision and Pattern Recognition (CVPR). Human Communicative Behavior Analysis Workshop</em>, Anchorage (USA), 2008
              <br>
              <a href="data/2008-CVPRW.bib.txt" target="_blank">[bibtex]</a>&nbsp;&nbsp;<a href="https://drive.google.com/open?id=0B5fAXmGH22uZRjZMM2NQZzM2Vk0" target="_blank">[poster]</a>
              <p></p>
              <p>TL;DR. Estimate the spatial focus of attention of a group of people leveraging their head orientation.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZWThIZV8zVm93UmM" target="_blank"><img src="images/2008-ICASSP.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZWThIZV8zVm93UmM" target="_blank">
                <papertitle>Audio-Driven Human Body Motion Analysis and Synthesis</papertitle>
              </a>
              <br>Ferda Ofli, <strong>Cristian Canton</strong>, Joelle Tilmanne, Yasemir Demir, Elif Bozkurt,<br>
              Yucel Yemez, Engin Erzin, Ahmet Murat Tekalp
              <br>
              <em>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, Las Vegas (USA), 2008.
              <br>
              <a href="data/2008-ICASSP.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Analysis and synthesis of motion patterns correlating audio and multiview information.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZUGp6VEVPZE1zRGc" target="_blank"><img src="images/2008-EURASIP.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZUGp6VEVPZE1zRGc" target="_blank">
                <papertitle>An Audio-Driven Dancing Avatar</papertitle>
              </a>
              <br>
              Ferda Ofli, <strong>Cristian Canton</strong>, Joelle Tilmanne, Yasmin Demir,<br>
              Elif Bozkurt, Yucel Yemez, Engin Erzin, Ahmet Murat Tekalp<br>
              <em><b>Journal</b> - EURASIP Journal on Advances on Signal Processing</em>, 2008.
              <br>
              <a href="data/2008-EURASIP.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. xxx.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="http://dx.doi.org/10.1109/SIU.2008.4632725" target="_blank"><img src="images/2008-ISPCAC.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://dx.doi.org/10.1109/SIU.2008.4632725" target="_blank">
                <papertitle>Analysis and synthesis of multiview audio-visual dance figures</papertitle>
              </a>
              <br>Ferda Ofli, Yasmin Demir,  <strong>Cristian Canton</strong>, Joelle Tilmanne Tilmanne, Korai Balci,<br>
              Elif Bozkurt, I. Kizoglu, Yucel Yemez, Engin Erzin, Ahmet Murat Tekalp,<br>
              Lale Akarun, T.A. Erdem
              <br>
              <em>IEEE Signal Processing, Communication and Applications Conference</em>, Aydin (Turkey), 2008.
              <br>
              <a href="data/2008-ISPCAC.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Human dance patterns generation directly from music.</p>
            </td>
          </tr>
          <!---- 2007 ------>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZTW54ZUpQNVozcDA" target="_blank"><img src="images/2007-CIVR.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZTW54ZUpQNVozcDA" target="_blank">
                <papertitle>Flexible Test-bed for Unusual Behavior Detection</papertitle>
              </a>
              <br>Istv√°n Petr√°s, Levente Kov√°cs, B. Uƒüur T√∂reyin, Csaba Beleznai, Zolt√°n Szl√°vik,<br>
              Uƒüur G√ºd√ºkbay, Yiƒüithan Dedeoƒülu, L√°szl√≥ Havasi, Enis Cetin, <a href="https://imatge.upc.edu/web/people/montse-pardas">Montse Pard√†s</a>,<br>
              Tam√°s Szir√°nyi, <strong>Cristian Canton</strong>
              <br>
              <em>ACM International Conference on Image and Video Retrieval (CIVR)</em>, Amsterdam (The Nederlands), 2007.
              <br>
              <a href="data/2007-CIVR.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. A system to combine multiple vision signals to detect unusual/outlier behaviors.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZX1hMNjMxNXpzd3M" target="_blank"><img src="images/2007-ClearHead.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZX1hMNjMxNXpzd3M" target="_blank">
                <papertitle>Head Orientation Estimation using Particle Filtering in Multiview Scenarios</papertitle>
              </a>
              <br>
              <strong>Cristian Canton</strong>,
              <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>,
              <a href="https://imatge.upc.edu/web/people/montse-pardas">Montse Pard√†s</a>
              <br>
              <em>Classification of Events, Activities and Relationships Evaluation and Workshop (CLEAR)</em>, Baltimore (USA), 2007.
              <br>
              <a href="data/2007-ClearHead.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Define head templates and use them in a particle filtering context to estimate head position.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZN0hJbXhlS25Bajg" target="_blank"><img src="images/2007-CLEAR.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZN0hJbXhlS25Bajg" target="_blank">
                <papertitle>Multi-Person Tracking Strategies Based on Voxel Analysis</papertitle>
              </a>
              <br>
              <strong>Cristian Canton</strong>,
              Jordi Salvador,
              <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>
              <br>
              <em>Classification of Events, Activities and Relationships Evaluation and Workshop (CLEAR)</em>, Baltimore (USA), 2007.
              <br>
              <a href="data/2007-CLEAR.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Comparison of two tracking aproaches (heuristic vs particle filtering) on the CLEAR dataset.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZVnNobEt2TlQySjg" target="_blank"><img src="images/2007-ICASSPTracking.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZVnNobEt2TlQySjg" target="_blank">
                <papertitle>Multi-Person 3D Tracking with Particle Filters on Voxels</papertitle>
              </a>
              <br>
              Adolfo L√≥pez,
              <strong>Cristian Canton</strong>,
              <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>
              <br>
              <em>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, Honolulu (USA), 2007.
              <br>
              <a href="data/2007-ICASSPTracking.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Person tracking using particle filters directly on voxels, handling multiple objects and delivering high accuracy tracking.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZa1JGQ25aWkViUnM" target="_blank"><img src="images/2007-EURASIPHead.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZa1JGQ25aWkViUnM" target="_blank">
                <papertitle>Audiovisual Head Orientation Estimation with Particle Filters in Multisensor Scenarions</papertitle>
              </a>
              <br>
              <strong>Cristian Canton</strong>,
              Carlos Segura,
              <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>,
              <a href="https://imatge.upc.edu/web/people/montse-pardas">Montse Pard√†s</a>,
              Javier Hernando
              <br>
              <em><b>Journal</b> - EURASIP Journal on Advances on Signal Processing</em>, 2007.
              <br>
              <a href="data/2007-EURASIPHead.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. xxxx.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZZTRxdVBwRmNBUEU" target="_blank"><img src="images/2007-ICASSPHead.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZZTRxdVBwRmNBUEU" target="_blank">
                <papertitle>Multimodal Head Orientation Towards Attention Tracking in SmartRooms</papertitle>
              </a>
              <br>
              Carlos Segura,
              <strong>Cristian Canton</strong>,
              Alberto Abad,
              <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>,
              Javier Hernando
              <br>
              <em>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, Honolulu (USA), 2007.
              <br>
              <a href="data/2007-ICASSPHead.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Head pose estimation using both multicamera and microphone arrays signals increase robustness and accuracy of estimation.</p>
            </td>
          </tr>
          <!---- 2006 ------>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZUTh5VVN5bkpCSm8" target="_blank"><img src="images/2006-EUSIPCO.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZUTh5VVN5bkpCSm8" target="_blank">
                <papertitle>Human Model and Motion Based 3D Action Recognition in Multiple View Scenarios</papertitle>
              </a>
              <br>
              <strong>Cristian Canton</strong>,
              <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>,
              <a href="https://imatge.upc.edu/web/people/montse-pardas">Montse Pard√†s</a>
              <br>
              <em>European Signal Processing Conference (EUSIPCO)</em>, Florence (Italy), 2006.
              <br>
              <a href="data/2006-EUSIPCO.bib.txt" target="_blank">[bibtex]</a>&nbsp&nbsp
              <a href="https://drive.google.com/file/d/0B5fAXmGH22uZa0ZLR3lYWUNKS0E/view" target="_blank">[slides]</a>
              <p></p>
              <p>TL;DR. Analyze 3D volumes using a human body model to recognize actions in multicamera scenarios.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZejdhSHJlY0RaZlU" target="_blank"><img src="images/2006-AVR.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZejdhSHJlY0RaZlU" target="_blank">
                <papertitle>3D Human Action Recognition in Multiple View Scenarios</papertitle>
              </a>
              <br>
              <strong>Cristian Canton</strong>,
              <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>,
              <a href="https://imatge.upc.edu/web/people/montse-pardas">Montse Pard√†s</a>,
              Mehmet Emre Sargin,
              <a href="http://home.ku.edu.tr/~mtekalp/">Ahmet Murat Tekalp</a>,
              <br>
              <em>Jornades de Recerca en Autom√†tica, Visi√≥ i Rob√≤tica (AVR)</em>, Barcelona (Spain), 2006.
              <br>
              <a href="data/2006-AVR.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Use multiple cameras to create voxels and recognize actions directly from the 3D.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZMHJhTk1UcmxsNWc" target="_blank"><img src="images/2006-CLEARHeadPose.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZMHJhTk1UcmxsNWc" target="_blank">
                <papertitle>Head Pose Detection Based on Fusion of Multiple Viewpoint Information</papertitle>
              </a>
              <br>
              <strong>Cristian Canton</strong>,
              <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>,
              <a href="https://imatge.upc.edu/web/people/montse-pardas">Montse Pard√†s</a>
              <br>
              <em>Classification of Events, Activities and Relationships Evaluation and Workshop (CLEAR)</em>, Southampton (UK), 2006.
              <br>
              <a href="data/2006-CLEARHeadPose.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Head position estimation using multiview face appearance and its evaluation against the CLEAR dataset.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZcDh4dTkzd3RqRUE" target="_blank"><img src="images/2006-CLEAR.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZcDh4dTkzd3RqRUE" target="_blank">
                <papertitle>UPC Audio, Video and Multimodal Person Tracking Systems in the CLEAR Evaluation Campaign</papertitle>
              </a>
              <br>
              <a href="https://www.hlt.inesc-id.pt/wiki/index.php/Alberto_Abad_Gareta">Alberto Abad</a>,
              <strong>Cristian Canton</strong>,
              Carlos Segura,
              Jos√© Luis Landabaso,
              Dusan Macho,<br>
              <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>,
              Javier Hernando,
              <a href="https://imatge.upc.edu/web/people/montse-pardas">Montse Pard√†s</a>,
              Climent Nadeu
              <br>
              <em>Classification of Events, Activities and Relationships Evaluation and Workshop (CLEAR)</em>, Southampton (UK), 2006.
              <br>
              <a href="data/2006-CLEAR.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Multiple modalities always improve tracking, specially of humans within a SmartRoom.</p>
            </td>
          </tr>
          <!---- 2005 ------>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZM2FlZkZFWmYwdEk" target="_blank"><img src="images/2005-ICCV-FaceOrientation.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZM2FlZkZFWmYwdEk" target="_blank">
                <papertitle>Fusion of Multiple Viewpoint Information Towards 3D Face Robust Orientation Detection</papertitle>
              </a>
              <br>
              <strong>Cristian Canton</strong>,
              <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>,
              <a href="https://imatge.upc.edu/web/people/montse-pardas">Montse Pard√†s</a>
              <br>
              <em>IEEE International Conference on Image Processing (ICIP)</em>, Genoa (Italy), 2005.
              <br>
              <a href="data/2005-ICCV-FaceOrientation.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Head orientation estimation by 3D backpropagation of detected skin patches on multiple views.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZWDlueTUtZWVvY2c" target="_blank"><img src="images/2005-MLMI-ProjectiveKalman.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZWDlueTUtZWVvY2c" target="_blank">
                <papertitle>Projective Kalman Filter: Multiocular Tracking of 3D Locations Towards Scene Understanding</papertitle>
              </a>
              <br>
              <strong>Cristian Canton</strong>,
              <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>,
              <a href="http://home.ku.edu.tr/~mtekalp/">Ahmet Murat Tekalp</a>,
              <a href="https://imatge.upc.edu/web/people/montse-pardas">Montse Pard√†s</a>
              <br>
              <em>Workshop on Multimodal Interaction and Related Machine Learning Algorithms (MLMI)</em>, Edinburgh (UK), 2005.
              <br>
              <a href="data/2005-MLMI-ProjectiveKalman.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Kalman filtering of 3D points directly from 2D projections; robust to occlusions by analyzing outliers. Simple yet effective!</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZZzF1RTFkYkk3TVE" target="_blank"><img src="images/2005-ICCS-BayesianCorrespondences.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZZzF1RTFkYkk3TVE" target="_blank">
                <papertitle>Towards a Bayesian Approach to Robust Finding Correspondences in Multiple View Geometry Environments</papertitle>
              </a>
              <br>
              <strong>Cristian Canton</strong>,
              <a href="https://imatge.upc.edu/web/people/josep-r-casas">Josep R. Casas</a>,
              <a href="https://imatge.upc.edu/web/people/montse-pardas">Montse Pard√†s</a>
              <br>
              <em>International Conference on Computer Science (ICCS). Workshop on Computer Graphics and Geometric Modelling</em>, Atlanta (USA), 2005.
              <br>
              <a href="data/2005-ICCS-BayesianCorrespondences.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Tracking multiple 3D points in a multiview scenario using a greedy Bayesian approach. Useful for low compute scenarios and SmartRooms.</p>
            </td>
          </tr>
          <!---- 2002 ------>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZRUxlU2tGeDVCV1k" target="_blank"><img src="images/2002-ICCVG-EvalAnisotropicDiffusion.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZRUxlU2tGeDVCV1k" target="_blank">
                <papertitle>Evaluation of the Efficiency of Robust Anisotropic Diffusion Schemes</papertitle>
              </a>
              <br>
              Bogdan Smolka, <strong>Cristian Canton</strong>, Marek Szczepanski, Konrad Wojciechowski
              <br>
              <em>International Conference on Computer Vision and Graphics (ICCVG)</em>, Zakopane (Poland), 2002.
              <br>
              <a href="data/2002-ICCVG-EvalAnisotropicDiffusion.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. Conductance functions used in anisotropic diffusion schemes may have relevant impact on the final result.</p>
            </td>
          </tr>
          <!---- 2001 ------>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZamJ5QXVxTWlTMHc" target="_blank"><img src="images/2001-IFAC-ImpulsiveNoise.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZamJ5QXVxTWlTMHc" target="_blank">
                <papertitle>Efficient Algorithm for Impulsive Noise Reduction</papertitle>
              </a>
              <br>
              Bogdan Smolka,
              <strong>Cristian Canton</strong>
              <br>
              <em>IFAC Workshop on Programable Devices and Systems</em>, Gliwice (Poland), 2001.
              <br>
              <a href="data/2001-IFAC-ImpulsiveNoise.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. An old shcool image filtering technique based on the maximization of the similarities between pixels in the filtering window.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle,horizontal-align:center">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZMzRONUFxSlNwYm8" target="_blank"><img src="images/2001-IFAC-AnisotropicDiffusion.png" style="border-style: none"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZMzRONUFxSlNwYm8" target="_blank">
                <papertitle>On the Efficiency of Anisotropic Diffusion Filtering Schemes</papertitle>
              </a>
              <br>
              Bogdan Smolka,
              <strong>Cristian Canton</strong>,
              Marko Marcevski, Zeina Torbey
              <br>
              <em>IFAC Workshop on Programable Devices and Systems</em>, Gliwice (Poland), 2001.
              <br>
              <a href="data/2001-IFAC-EfficiencyAnisotropy.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. When running anisotropic diffusion filters on images, not all conduction functions were born equal.</p>
            </td>
          </tr>
        </tbody></table>

        <!--- THESES -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Theses</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <center><a href="https://drive.google.com/open?id=0B5fAXmGH22uZMEoxX21uaG5jNnc" target="_blank">
                <img src="images/2009-PhDThesis.png" style="border-style: none"></a></center>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZMEoxX21uaG5jNnc" target="_blank">
                <papertitle>Human Motion Capture with Scalable Body Models</papertitle>
              </a>, PhD Thesis
              <br>
              <strong>Cristian Canton</strong>
              <br>
              <em>Technical University of Catalonia</em>, Barcelona (Spain), 2009.
              <br>
              <a href="data/2009-PhDThesis.bib.txt" target="_blank">[bibtex]</a>&nbsp;&nbsp;<a href="https://drive.google.com/open?id=0B5fAXmGH22uZYUxqRzdvN0RXMUk" target="_blank">[PDF (High Resolution)]</a>
              <p></p>
              <p>TL;DR. xxx</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <center><a href="https://drive.google.com/open?id=0B5fAXmGH22uZWi1MZUhEdmlIVnc" target="_blank">
                <img src="images/2003-MsThesis.png" style="border-style: none"></a></center>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=0B5fAXmGH22uZWi1MZUhEdmlIVnc" target="_blank">
                <papertitle>Texture and Edge Adaptive Weak Matching Pursuit</papertitle>
              </a>, MS Thesis
              <br>
              <strong>Cristian Canton</strong>
              <br>
              <em>√âcole Polytechnique F√©d√©rale de Lausanne (EPFL)</em>, Lausanne (Switzerland), 2003.
              <br>
              <a href="data/2003-MsThesis.bib.txt" target="_blank">[bibtex]</a>
              <p></p>
              <p>TL;DR. xxx</p>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
              <a href="http://iccv2021.thecvf.com/area-chairs">Area Chair, ICCV 2021</a><br>
              <a href="https://sites.google.com/view/mediaforensics2021">General chair, Workshop on Media Forensics @ CVPR 2021</a><br>
              <a href="http://wacv2021.thecvf.com/organizers">Program Chair, WACV 2021</a>
              <a href="https://sites.google.com/view/deepvision2020">Co-chair, DeepVision Workshop @ CVPR 2020</a><br>
              <a href="https://sites.google.com/view/wmediaforensics2020">General chair, Workshop on Media Forensics @ CVPR 2020</a><br>
              <a href="https://sites.google.com/view/deepvision2019">Co-chair, DeepVision Workshop @ CVPR 2019</a><br>
              <a href="https://sites.google.com/view/mediaforensics2019">General chair, Workshop on Media Forensics @ CVPR 2019</a><br>
              <a href="https://sites.google.com/view/wocm2018">General chair, Workshop on Objectionable Content and Misinformation @ ECCV 2018</a><br>
            </td>
          </tr>
          <!--<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cs188.jpg" alt="cs188">
            </td>
            <td width="75%" valign="center">
              <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
              <br>
              <br>
              <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
              <br>
              <br>
              <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
            </td>
          </tr>-->
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Like the style? Shamelessly borrowed from <a href="https://jonbarron.info">Jon Barron</a>'s site.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
